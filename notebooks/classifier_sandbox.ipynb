{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e511f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rga.data.diag_repr_graph_data_module import DiagonalRepresentationGraphDataModule\n",
    "from rga.data.graph_loaders import RealGraphLoader, SyntheticGraphLoader\n",
    "from rga.experiments.decorators import add_graphloader_args\n",
    "from rga.models.autoencoder_components import GraphEncoder\n",
    "from rga.models.edge_encoders import MemoryEdgeEncoder\n",
    "from rga.util.load_model import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealSaver(DiagonalRepresentationGraphDataModule):\n",
    "    graphloader_class = RealGraphLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_split = [0.7, 0.15, 0.15]\n",
    "train_val_test_permutation_split = [1, 0, 0.0]\n",
    "num_dataset_graph_permutations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8484d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RealSaver(\n",
    "    pickled_dataset_path='./datasets/imdb_multi_labels.pkl',\n",
    "    use_labels=True,\n",
    "    bfs=True,\n",
    "    deduplicate_train = False,\n",
    "    deduplicate_val_test = False,\n",
    "    batch_size=500000,\n",
    "    batch_size_val=500000,\n",
    "    batch_size_test=500000,\n",
    "    workers=0,\n",
    "    block_size=8,\n",
    "    subgraph_scheduler_name='none',\n",
    "    subgraph_scheduler_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737506ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'tb_logs/RecursiveGraphAutoencoder/version_8/checkpoints/epoch=77-step=1325-v1.ckpt'\n",
    "hparams_path = 'tb_logs/RecursiveGraphAutoencoder/version_8/hparams.yaml'\n",
    "hparams = load_hparams(hparams_path)\n",
    "encoder = GraphEncoder(edge_encoder_class = MemoryEdgeEncoder, **hparams)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "encoder_checkpoint = {\n",
    "    k.replace(\"encoder.edge_encoder.\", \"edge_encoder.\"): v\n",
    "    for (k, v) in checkpoint[\"state_dict\"].items()\n",
    "    if \"encoder\" in k\n",
    "}\n",
    "encoder.load_state_dict(encoder_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae821b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rga.models.classifier_components import MLPClassifier\n",
    "classifier_checkpoint_model = MLPClassifier(**hparams)\n",
    "\n",
    "classifier_checkpoint = {\n",
    "    k.replace(\"classifier.nn.\", \"nn.\"): v\n",
    "    for (k, v) in checkpoint[\"state_dict\"].items()\n",
    "    if \"class\" in k\n",
    "}\n",
    "\n",
    "classifier_checkpoint_model.load_state_dict(classifier_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adace3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca36863",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = next(iter(dataset.train_dataloader()))\n",
    "train_batch_labels = train_batch[3]\n",
    "print(len(train_batch_labels))\n",
    "\n",
    "val_batch = next(iter(dataset.val_dataloader()[0]))\n",
    "val_batch_labels = val_batch[3]\n",
    "print(len(val_batch_labels))\n",
    "\n",
    "test_batch = next(iter(dataset.test_dataloader()[0]))\n",
    "test_batch_labels = test_batch[3]\n",
    "print(len(test_batch_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_X = encoder(train_batch).detach().numpy()\n",
    "val_batch_X = encoder(val_batch).detach().numpy()\n",
    "test_batch_X = encoder(test_batch).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7064650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_checkpoint_model(torch.tensor(val_batch_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccfff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = torch.argmax(classifier_checkpoint_model(torch.tensor(val_batch_X)), dim=1)\n",
    "print(classification_report(val_batch_labels-1, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d166cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch import nn\n",
    "from rga.models.utils.layers import sequential_from_layer_sizes\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class EmbeddingAE(LightningModule):\n",
    "    def __init__(self, embedding_size, layers):\n",
    "        super().__init__()\n",
    "        self.compressing_layer = torch.argmin(torch.tensor(layers))\n",
    "        self.nn = sequential_from_layer_sizes(embedding_size, embedding_size, layers)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "    def get_compressed_embeddings(self, x):\n",
    "        return self.nn[:self.compressing_layer*2+1](x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch, self(batch))\n",
    "        self.log(\"loss/train\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch, self(batch))\n",
    "        self.log(\"loss/val\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=False)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch_lightning as pl\n",
    "# from torch import nn\n",
    "# import torch\n",
    "\n",
    "\n",
    "\n",
    "# class VAE(pl.LightningModule):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # self.save_hyperparameters()\n",
    "\n",
    "#         # encoder, decoder\n",
    "#         self.encoder = sequential_from_layer_sizes(256, 32, [128, 64])\n",
    "#         self.decoder = sequential_from_layer_sizes(16, 256, [32, 64, 128])\n",
    "#         self.loss = nn.MSELoss()\n",
    "#         # distribution parameters\n",
    "#         self.fc_mu = nn.Linear(32, 16)\n",
    "#         self.fc_var = nn.Linear(32, 16)\n",
    "\n",
    "#         # for the gaussian likelihood\n",
    "#         self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "#     def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "#         scale = torch.exp(logscale)\n",
    "#         mean = x_hat\n",
    "#         dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "#         # measure prob of seeing image under p(x|z)\n",
    "#         log_pxz = dist.log_prob(x)\n",
    "#         return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "#     def kl_divergence(self, z, mu, std):\n",
    "#         # --------------------------\n",
    "#         # Monte carlo KL divergence\n",
    "#         # --------------------------\n",
    "#         # 1. define the first two probabilities (in this case Normal for both)\n",
    "#         p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "#         q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "#         # 2. get the probabilities from the equation\n",
    "#         log_qzx = q.log_prob(z)\n",
    "#         log_pz = p.log_prob(z)\n",
    "\n",
    "#         # kl\n",
    "#         kl = (log_qzx - log_pz)\n",
    "#         kl = kl.sum(-1)\n",
    "#         return kl\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x = batch\n",
    "\n",
    "#         # encode x to get the mu and variance parameters\n",
    "#         x_encoded = self.encoder(x)\n",
    "#         mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "#         # sample z from q\n",
    "#         std = torch.exp(log_var / 2)\n",
    "#         q = torch.distributions.Normal(mu, std)\n",
    "#         z = q.rsample()\n",
    "\n",
    "#         # decoded\n",
    "#         x_hat = self.decoder(z)\n",
    "\n",
    "#         # reconstruction loss\n",
    "#         recon_loss = self.loss(x_hat, x)#self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "\n",
    "#         # kl\n",
    "#         kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "#         # elbo\n",
    "#         elbo = (kl - recon_loss)\n",
    "#         elbo = elbo.mean()\n",
    "\n",
    "#         self.log_dict({\n",
    "#             'elbo': elbo,\n",
    "#             'kl': kl.mean(),\n",
    "#             'recon_loss': recon_loss.mean(),\n",
    "#             'reconstruction': recon_loss.mean(),\n",
    "#             'kl': kl.mean(),\n",
    "#         })\n",
    "\n",
    "#         return elbo\n",
    "\n",
    "#     def get_compressed_embeddings(self, batch):\n",
    "#         x_encoded = self.encoder(batch)\n",
    "#         return self.fc_mu(x_encoded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ae = VAE()#EmbeddingAE(256, [128, 64, 32, 16, 32, 64, 128])\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# dataloaders_ae = {\n",
    "#     'train':DataLoader(train_batch_X, batch_size=32, num_workers=0),\n",
    "#     'val':DataLoader(val_batch_X, batch_size=32, num_workers=0),\n",
    "#     'test':DataLoader(test_batch_X, batch_size=32, num_workers=0)\n",
    "# }\n",
    "\n",
    "\n",
    "# trainer = Trainer(max_epochs=50, log_every_n_steps=5) #, callbacks=[EarlyStopping(monitor=\"loss/val\")]\n",
    "# trainer.fit(ae, train_dataloaders=dataloaders_ae.get('train'), val_dataloaders=dataloaders_ae.get('val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch_X_compressed = ae.get_compressed_embeddings(torch.tensor(train_batch_X)).detach().numpy()\n",
    "# val_batch_X_compressed = ae.get_compressed_embeddings(torch.tensor(val_batch_X)).detach().numpy()\n",
    "# test_batch_X_compressed = ae.get_compressed_embeddings(torch.tensor(test_batch_X)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch_X_compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5665ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb01831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch import nn\n",
    "from rga.models.utils.layers import sequential_from_layer_sizes\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "from rga.models.base import BaseModel\n",
    "from rga.models.utils.getters import * \n",
    "\n",
    "class EmbeddingClassifier(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size: int,\n",
    "        class_count: int,\n",
    "        classifier_hidden_layer_sizes,\n",
    "        classifier_activation_function: str,\n",
    "        classifier_dropout: float,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_size = embedding_size\n",
    "        input_size = embedding_size\n",
    "\n",
    "        activation_f = get_activation_function(classifier_activation_function)\n",
    "        self.class_count = class_count\n",
    "        output_function = nn.Sigmoid if class_count == 2 else nn.Softmax\n",
    "\n",
    "        self.gru_depth = 8\n",
    "\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size = 0,\n",
    "            hidden_size = embedding_size,\n",
    "            num_layers = 1,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        self.nn = sequential_from_layer_sizes(\n",
    "            input_size,\n",
    "            class_count if class_count != 2 else 1,\n",
    "            classifier_hidden_layer_sizes,\n",
    "            activation_f,\n",
    "            output_function=output_function,\n",
    "            dropout=classifier_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, graphs: Tensor) -> Tensor:\n",
    "        gru_in = torch.empty(size=[graphs.shape[0], self.gru_depth, 0])\n",
    "        gru_h_0 = graphs[None, :, :]\n",
    "        gru_out, gru_hidden = self.gru(gru_in, gru_h_0)\n",
    "        graphs = gru_hidden[0]\n",
    "\n",
    "        return self.nn(graphs)\n",
    "\n",
    "    def step(self, batch, metrics: List = []) -> Tensor:\n",
    "        y_pred = self(batch[:, :-1])\n",
    "        labels = (batch[:, -1] - 1).long()\n",
    "\n",
    "        if self.class_count == 2:\n",
    "            loss = self.loss_function(y_pred[:, 0], labels.float())\n",
    "            y_pred_labels = torch.round(y_pred[:, 0]).int()\n",
    "        else:\n",
    "            loss = self.loss_function(y_pred, labels)\n",
    "            y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric(y_pred_labels, labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "model = EmbeddingClassifier(\n",
    "    embedding_size = 104, \n",
    "    class_count = 3, \n",
    "    classifier_hidden_layer_sizes = [128, 64], \n",
    "    classifier_activation_function = 'ReLU', \n",
    "    classifier_dropout = 0.1, \n",
    "    loss_function='CrossEntropy',\n",
    "    metrics=['Accuracy'],\n",
    "    metric_update_interval=1,\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "data_loader_embedding_classifier = DataLoader(torch.cat([torch.tensor(train_batch_X), train_batch_labels[:, None]], axis = 1), batch_size=32)\n",
    "data_loader_val_embedding_classifier = DataLoader(torch.cat([torch.tensor(val_batch_X), val_batch_labels[:, None]], axis = 1), batch_size=32)\n",
    "\n",
    "\n",
    "trainer = Trainer(max_epochs = 100, check_val_every_n_epoch=5,)\n",
    "trainer.fit(model, train_dataloaders=data_loader_embedding_classifier, val_dataloaders=data_loader_val_embedding_classifier)\n",
    "# trainer.test(model, dataloaders=data_loader_val_embedding_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce02490",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = torch.argmax(model(torch.tensor(val_batch_X)), dim=1)\n",
    "print(classification_report(val_batch_labels-1, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0933e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524db657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn_model = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, min_samples_split=4)\n",
    "sklearn_model = GradientBoostingClassifier(min_samples_leaf=5, min_samples_split=4)\n",
    "# sklearn_model = SVC()\n",
    "# sklearn_model = MLPClassifier(hidden_layer_sizes=[16, 16, 16, 16, 16, 16], random_state=1,max_iter=500)\n",
    "sklearn_model.fit(train_batch_X, train_batch_labels)\n",
    "train_batch_labels_pred = sklearn_model.predict(train_batch_X)\n",
    "print(classification_report(train_batch_labels, train_batch_labels_pred))\n",
    "\n",
    "val_batch_labels_pred = sklearn_model.predict(val_batch_X)\n",
    "print(classification_report(val_batch_labels, val_batch_labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38776c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c37224fb4409952eb251e31ba483053a30ac3ecaa917b50e887cf90f0c69f7d5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
