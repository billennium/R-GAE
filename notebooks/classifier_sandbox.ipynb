{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f420238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adam/phd/recurrent-graph-autoencoder\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8b0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rga.data.diag_repr_graph_data_module import DiagonalRepresentationGraphDataModule\n",
    "from rga.data.graph_loaders import RealGraphLoader, SyntheticGraphLoader\n",
    "from rga.experiments.decorators import add_graphloader_args\n",
    "from rga.models.autoencoder_components import GraphEncoder\n",
    "from rga.models.edge_encoders import MemoryEdgeEncoder\n",
    "from rga.util.load_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fd2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealSaver(DiagonalRepresentationGraphDataModule):\n",
    "    graphloader_class = RealGraphLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8592e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_split = [0.7, 0.15, 0.15]\n",
    "train_val_test_permutation_split = [1, 0, 0.0]\n",
    "num_dataset_graph_permutations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d8484d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a382c79317846d389892ab19a96479e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading edges: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic of set:  Full original dataset\n",
      "             Dataset size : 1500\n",
      "                   Labels : True\n",
      "           Min node count : 7\n",
      "       Average node count : 13.0\n",
      "           Max node count : 89\n",
      "           Min edge count : 12.0\n",
      "       Average edge count : 65.94\n",
      "           Max edge count : 1467.0\n",
      "     Min filling fraction : 0.13\n",
      " Average filling fraction : 0.77\n",
      "     Max filling fraction : 1.0\n",
      "          Label \"1\" count : 500\n",
      "          Label \"2\" count : 500\n",
      "          Label \"3\" count : 500\n",
      "----------------------------------------------------------------\n",
      "Statistic of set:  Train dataset\n",
      "             Dataset size : 1050\n",
      "                   Labels : True\n",
      "           Min node count : 7\n",
      "       Average node count : 13.27\n",
      "           Max node count : 78\n",
      "           Min edge count : 12.0\n",
      "       Average edge count : 68.94\n",
      "           Max edge count : 982.0\n",
      "     Min filling fraction : 0.13\n",
      " Average filling fraction : 0.77\n",
      "     Max filling fraction : 1.0\n",
      "          Label \"1\" count : 352\n",
      "          Label \"2\" count : 351\n",
      "          Label \"3\" count : 347\n",
      "----------------------------------------------------------------\n",
      "Statistic of set:  Validation dataset 0\n",
      "             Dataset size : 225\n",
      "                   Labels : True\n",
      "           Min node count : 7\n",
      "       Average node count : 12.23\n",
      "           Max node count : 47\n",
      "           Min edge count : 12.0\n",
      "       Average edge count : 55.11\n",
      "           Max edge count : 626.0\n",
      "     Min filling fraction : 0.2\n",
      " Average filling fraction : 0.79\n",
      "     Max filling fraction : 1.0\n",
      "          Label \"1\" count : 74\n",
      "          Label \"2\" count : 80\n",
      "          Label \"3\" count : 71\n",
      "----------------------------------------------------------------\n",
      "Statistic of set:  Test dataset 0\n",
      "             Dataset size : 225\n",
      "                   Labels : True\n",
      "           Min node count : 7\n",
      "       Average node count : 12.52\n",
      "           Max node count : 89\n",
      "           Min edge count : 12.0\n",
      "       Average edge count : 62.75\n",
      "           Max edge count : 1467.0\n",
      "     Min filling fraction : 0.14\n",
      " Average filling fraction : 0.77\n",
      "     Max filling fraction : 1.0\n",
      "          Label \"1\" count : 74\n",
      "          Label \"2\" count : 69\n",
      "          Label \"3\" count : 82\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed54755771f94e33a3a12c1cb0c9f4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preparing dataset train for autoencoder:   0%|          | 0/1050 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4329533fc24f51894e19007486b858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preparing dataset val 0 for autoencoder:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c1b01f8bc84ecdaeeef0092dc458c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preparing dataset test 0 for autoencoder:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = RealSaver(\n",
    "    datasets_dir='/home/adam/phd/recurrent-graph-autoencoder/datasets',\n",
    "    dataset_name='IMDB-MULTI',\n",
    "    use_labels=True,\n",
    "    max_graph_size=None,\n",
    "    num_dataset_graph_permutations=1, \n",
    "    train_val_test_split=train_val_test_split, \n",
    "    train_val_test_permutation_split=train_val_test_permutation_split,\n",
    "    # save_dataset_to_pickle=to_save_path+'/'+dataset_name+'/'+str(i)+'.pkl',\n",
    "    bfs=True,\n",
    "    deduplicate_train = False,\n",
    "    deduplicate_val_test = False,\n",
    "    batch_size=500000,\n",
    "    batch_size_val=500000,\n",
    "    batch_size_test=500000,\n",
    "    workers=0,\n",
    "    block_size=6,\n",
    "    subgraph_scheduler_name='none',\n",
    "    subgraph_scheduler_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b105dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RealSaver at 0x7f26215a9760>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "737506ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = '/home/adam/phd/recurrent-graph-autoencoder/tb_logs/RecurrentGraphAutoencoder/IMDB-MULTI/version_1/checkpoints/epoch=224-step=9674-v1.ckpt'\n",
    "hparams_path = '/home/adam/phd/recurrent-graph-autoencoder/tb_logs/RecurrentGraphAutoencoder/IMDB-MULTI/version_1/hparams.yaml'\n",
    "hparams = load_hparams(hparams_path)\n",
    "encoder = GraphEncoder(edge_encoder_class = MemoryEdgeEncoder, **hparams)\n",
    "# (\n",
    "#     edge_encoder_class = MemoryEdgeEncoder,\n",
    "#     embedding_size = 104,\n",
    "#     edge_size = 1,\n",
    "#     block_size= 6,\n",
    "#     loss_function = 'BCEWithLogits',\n",
    "#     loss_weight = None,\n",
    "#     learning_rate = '0.0001',\n",
    "#     optimizer = 'AdamWAMSGrad',\n",
    "#     lr_scheduler_name = 'NoSched',\n",
    "#     lr_scheduler_params = {},\n",
    "#     lr_scheduler_metric = 'loss/train_avg',\n",
    "#     metrics = [],\n",
    "#     encoder_hidden_layer_sizes=[1024, 768],\n",
    "#     encoder_activation_function='ELU'\n",
    "# )\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "encoder_checkpoint = {\n",
    "    k.replace(\"encoder.edge_encoder.\", \"edge_encoder.\"): v\n",
    "    for (k, v) in checkpoint[\"state_dict\"].items()\n",
    "    if \"encoder\" in k\n",
    "}\n",
    "encoder.load_state_dict(encoder_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca36863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "225\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "train_batch = next(iter(dataset.train_dataloader()))\n",
    "train_batch_labels = train_batch[3]\n",
    "print(len(train_batch_labels))\n",
    "\n",
    "val_batch = next(iter(dataset.val_dataloader()[0]))\n",
    "val_batch_labels = val_batch[3]\n",
    "print(len(val_batch_labels))\n",
    "\n",
    "test_batch = next(iter(dataset.test_dataloader()[0]))\n",
    "test_batch_labels = test_batch[3]\n",
    "print(len(test_batch_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a69f4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_X = encoder(train_batch).detach().numpy()\n",
    "val_batch_X = encoder(val_batch).detach().numpy()\n",
    "test_batch_X = encoder(test_batch).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7064650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bdcd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6d166cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch import nn\n",
    "from rga.models.utils.layers import sequential_from_layer_sizes\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class EmbeddingAE(LightningModule):\n",
    "    def __init__(self, embedding_size, layers):\n",
    "        super().__init__()\n",
    "        self.compressing_layer = torch.argmin(torch.tensor(layers))\n",
    "        self.nn = sequential_from_layer_sizes(embedding_size, embedding_size, layers)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "    def get_compressed_embeddings(self, x):\n",
    "        return self.nn[:self.compressing_layer*2+1](x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch, self(batch))\n",
    "        self.log(\"loss/train\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch, self(batch))\n",
    "        self.log(\"loss/val\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=False)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7abb35ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/adam/Oprogramowanie/anaconda3/envs/rga/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1580: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "/home/adam/Oprogramowanie/anaconda3/envs/rga/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:116: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 43.2 K\n",
      "1 | decoder | Sequential | 44.0 K\n",
      "2 | loss    | MSELoss    | 0     \n",
      "3 | fc_mu   | Linear     | 528   \n",
      "4 | fc_var  | Linear     | 528   \n",
      "---------------------------------------\n",
      "88.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "88.3 K    Total params\n",
      "0.353     Total estimated model params size (MB)\n",
      "/home/adam/Oprogramowanie/anaconda3/envs/rga/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988662c99b5843e08e52c8097f309a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "        # encoder, decoder\n",
    "        self.encoder = sequential_from_layer_sizes(256, 32, [128, 64])\n",
    "        self.decoder = sequential_from_layer_sizes(16, 256, [32, 64, 128])\n",
    "        self.loss = nn.MSELoss()\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(32, 16)\n",
    "        self.fc_var = nn.Linear(32, 16)\n",
    "\n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "        scale = torch.exp(logscale)\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "        # measure prob of seeing image under p(x|z)\n",
    "        log_pxz = dist.log_prob(x)\n",
    "        return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        # decoded\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = self.loss(x_hat, x)#self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "\n",
    "        # kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'kl': kl.mean(),\n",
    "            'recon_loss': recon_loss.mean(),\n",
    "            'reconstruction': recon_loss.mean(),\n",
    "            'kl': kl.mean(),\n",
    "        })\n",
    "\n",
    "        return elbo\n",
    "\n",
    "    def get_compressed_embeddings(self, batch):\n",
    "        x_encoded = self.encoder(batch)\n",
    "        return self.fc_mu(x_encoded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ae = VAE()#EmbeddingAE(256, [128, 64, 32, 16, 32, 64, 128])\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "dataloaders_ae = {\n",
    "    'train':DataLoader(train_batch_X, batch_size=32, num_workers=0),\n",
    "    'val':DataLoader(val_batch_X, batch_size=32, num_workers=0),\n",
    "    'test':DataLoader(test_batch_X, batch_size=32, num_workers=0)\n",
    "}\n",
    "\n",
    "\n",
    "trainer = Trainer(max_epochs=50, log_every_n_steps=5) #, callbacks=[EarlyStopping(monitor=\"loss/val\")]\n",
    "trainer.fit(ae, train_dataloaders=dataloaders_ae.get('train'), val_dataloaders=dataloaders_ae.get('val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb58cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_X_compressed = ae.get_compressed_embeddings(torch.tensor(train_batch_X)).detach().numpy()\n",
    "val_batch_X_compressed = ae.get_compressed_embeddings(torch.tensor(val_batch_X)).detach().numpy()\n",
    "test_batch_X_compressed = ae.get_compressed_embeddings(torch.tensor(test_batch_X)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8ed08fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_X_compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0933e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "524db657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.34      0.44       352\n",
      "           2       0.56      0.70      0.62       351\n",
      "           3       0.53      0.66      0.59       347\n",
      "\n",
      "    accuracy                           0.56      1050\n",
      "   macro avg       0.58      0.57      0.55      1050\n",
      "weighted avg       0.58      0.56      0.55      1050\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.22      0.30        74\n",
      "           2       0.46      0.56      0.51        80\n",
      "           3       0.44      0.58      0.50        71\n",
      "\n",
      "    accuracy                           0.45       225\n",
      "   macro avg       0.46      0.45      0.43       225\n",
      "weighted avg       0.46      0.45      0.44       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, min_samples_split=4)\n",
    "model = GradientBoostingClassifier(min_samples_leaf=5, min_samples_split=4)\n",
    "# model = SVC()\n",
    "# model = MLPClassifier(hidden_layer_sizes=[16, 16, 16, 16, 16, 16], random_state=1,max_iter=500)\n",
    "model.fit(train_batch_X_compressed, train_batch_labels)\n",
    "train_batch_labels_pred = model.predict(train_batch_X_compressed)\n",
    "print(classification_report(train_batch_labels, train_batch_labels_pred))\n",
    "\n",
    "val_batch_labels_pred = model.predict(val_batch_X_compressed)\n",
    "print(classification_report(val_batch_labels, val_batch_labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38776c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c37224fb4409952eb251e31ba483053a30ac3ecaa917b50e887cf90f0c69f7d5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
