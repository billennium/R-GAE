{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920eef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from rga.data.diag_repr_graph_data_module import DiagonalRepresentationGraphDataModule\n",
    "from rga.data.graph_loaders import RealGraphLoader, SyntheticGraphLoader\n",
    "# from rga.experiments.decorators import add_graphloader_args\n",
    "from rga.models.autoencoder_components import GraphEncoder\n",
    "from rga.models.edge_encoders import MemoryEdgeEncoder\n",
    "from rga.util.load_model import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import *\n",
    "# from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_folder = '/home/jgrzechocinski/recurrent-graph-autoencoder/best_checkpoints/'\n",
    "datasets = [\n",
    "    'COLLAB',\n",
    "    'IMDB-BINARY',\n",
    "    'IMDB-MULTI',\n",
    "    'REDDIT-BINARY'\n",
    "]\n",
    "dataset_folder = '/usr/local/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad42895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls '/home/jgrzechocinski/recurrent-graph-autoencoder/best_checkpoints/COLLAB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1471546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_index = 1\n",
    "# pickle_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealSaver(DiagonalRepresentationGraphDataModule):\n",
    "    graphloader_class = RealGraphLoader\n",
    "    \n",
    "def prepare_model(model_path, hparams):\n",
    "    encoder = GraphEncoder(edge_encoder_class = MemoryEdgeEncoder, **hparams)\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    encoder_checkpoint = {\n",
    "        k.replace(\"encoder.edge_encoder.\", \"edge_encoder.\"): v\n",
    "        for (k, v) in checkpoint[\"state_dict\"].items()\n",
    "        if \"encoder\" in k\n",
    "    }\n",
    "    encoder.load_state_dict(encoder_checkpoint)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def prepare_dataset(dataset_path, hparams):\n",
    "    return RealSaver(\n",
    "        pickled_dataset_path=dataset_path,\n",
    "        use_labels=True,\n",
    "        bfs=True,\n",
    "        deduplicate_train = False,\n",
    "        deduplicate_val_test = False,\n",
    "        batch_size=32,\n",
    "        batch_size_val=32,\n",
    "        batch_size_test=32,\n",
    "        workers=0,\n",
    "        block_size=hparams['block_size'],\n",
    "        subgraph_scheduler_name='none',\n",
    "        subgraph_scheduler_params={}\n",
    "    )\n",
    "\n",
    "def get_embeddings(model, dataloader, N = 1):\n",
    "    data_iterator = iter(dataloader)\n",
    "    X = []\n",
    "    Y = []\n",
    "    sizes = []\n",
    "    avg_degree = []\n",
    "    for i, batch in enumerate(tqdm(data_iterator)):\n",
    "        X.append(model(batch).detach().numpy())\n",
    "        Y.append(batch[3])\n",
    "        sizes.append(batch[2])\n",
    "\n",
    "    return np.concatenate(X), np.concatenate(Y), np.concatenate(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413adba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_model(hparams_path, model_path, dataset_path, PCA_dim = None):\n",
    "    hparams = load_hparams(hparams_path)\n",
    "\n",
    "    model = prepare_model(model_path, hparams)\n",
    "    dataset = prepare_dataset(dataset_path, hparams)\n",
    "\n",
    "    train_X, train_Y, train_sizes = get_embeddings(model, dataset.train_dataloader(), N = 128)\n",
    "    val_X, val_Y, train_sizes = get_embeddings(model, dataset.val_dataloader()[0], N = 128)\n",
    "\n",
    "    if PCA_dim is not None:\n",
    "        pca = PCA(n_components=PCA_dim)\n",
    "        pca.fit(train_X)\n",
    "        train_X = pca.transform(train_X)\n",
    "        val_X = pca.transform(val_X)\n",
    "        # test_X_reduced = pca.transform(test_X)\n",
    "\n",
    "    sklearn_models = {\n",
    "        'NB': GaussianNB(),\n",
    "        'SVM': SVC(),\n",
    "        'Logistic regression': LogisticRegression(),\n",
    "        'xgboost': GradientBoostingClassifier(min_samples_leaf=10, verbose=False),\n",
    "    }\n",
    "\n",
    "    stats = {\n",
    "        'train': {},\n",
    "        'val': {},\n",
    "        'test': {}\n",
    "    }\n",
    "    for name, sklearn_model in sklearn_models.items():\n",
    "        sklearn_model.fit(train_X, train_Y)\n",
    "        train_preds = sklearn_model.predict(train_X)\n",
    "        val_preds = sklearn_model.predict(val_X)\n",
    "    #     test_preds = sklearn_model.predict(test_X_reduced)\n",
    "\n",
    "#         print(name)\n",
    "#         display(pd.DataFrame([val_Y, val_preds], index=['True', 'Pred']).transpose().value_counts())\n",
    "#         print(classification_report(val_Y, val_preds))\n",
    "        \n",
    "        stats['train'].update({\n",
    "            name: accuracy_score(train_Y, train_preds),\n",
    "        })\n",
    "        stats['val'].update({\n",
    "            name: accuracy_score(val_Y, val_preds),\n",
    "        })\n",
    "    #     stats['test'].update({\n",
    "    #         name: accuracy_score(test_Y, test_preds),\n",
    "    #     })\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ca9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results = {\n",
    "    'train':{dataset:{} for dataset in datasets}, \n",
    "    'val':{dataset:{} for dataset in datasets}, \n",
    "    'test':{dataset:{} for dataset in datasets}, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f573e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'IMDB-BINARY'\n",
    "for pickle in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path)\n",
    "    \n",
    "    selected_stats = {(pickle, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da35d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd2c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'IMDB-MULTI'\n",
    "for pickle in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path)\n",
    "    \n",
    "    selected_stats = {(pickle, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdddaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'REDDIT-BINARY'\n",
    "for pickle in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, PCA_dim = 256)\n",
    "    \n",
    "    selected_stats = {(pickle, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'COLLAB'\n",
    "for pickle in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, PCA_dim = 256)\n",
    "    \n",
    "    selected_stats = {(pickle, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'REDDIT-MULTI-5K'\n",
    "# classification_results[dataset_name] = {}\n",
    "# for pickle in tqdm([0, 1, 2, 3, 4]):\n",
    "#     hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '_hparams.yaml'\n",
    "#     model_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '.ckpt'\n",
    "#     dataset_path = dataset_folder + dataset_name + '/' + str(pickle) + '.pkl'\n",
    "\n",
    "#     selected_stats = {(pickle, k):v for (k,v) in stats['train'].items()}\n",
    "#     classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "#     selected_stats = {(pickle, k):v for (k,v) in stats['val'].items()}\n",
    "#     classification_results['val'][dataset_name].update(selected_stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a158844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'REDDIT-MULTI-12K'\n",
    "# classification_results[dataset_name] = {}\n",
    "# for pickle in tqdm([0, 1, 2, 3, 4]):\n",
    "#     hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '_hparams.yaml'\n",
    "#     model_path = checkpoints_folder + dataset_name + '/' + str(pickle) + '.ckpt'\n",
    "#     dataset_path = dataset_folder + dataset_name + '/' + str(pickle) + '.pkl'\n",
    "\n",
    "#     selected_stats = {(pickle, k):v for (k,v) in stats['train'].items()}\n",
    "#     classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "#     selected_stats = {(pickle, k):v for (k,v) in stats['val'].items()}\n",
    "#     classification_results['val'][dataset_name].update(selected_stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca680af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c37224fb4409952eb251e31ba483053a30ac3ecaa917b50e887cf90f0c69f7d5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
