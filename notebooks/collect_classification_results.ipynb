{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920eef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from rga.data.diag_repr_graph_data_module import DiagonalRepresentationGraphDataModule\n",
    "from rga.data.graph_loaders import RealGraphLoader, SyntheticGraphLoader\n",
    "# from rga.experiments.decorators import add_graphloader_args\n",
    "from rga.models.autoencoder_components import GraphEncoder\n",
    "from rga.models.edge_encoders import MemoryEdgeEncoder\n",
    "from rga.util.load_model import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import *\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from rga.util.adjmatrix.diagonal_block_representation import diagonal_block_to_adj_matrix_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_folder = '/home/jgrzechocinski/recurrent-graph-autoencoder/best_checkpoints/'\n",
    "datasets = [\n",
    "    'COLLAB',\n",
    "    'IMDB-BINARY',\n",
    "    'IMDB-MULTI',\n",
    "    'REDDIT-BINARY'\n",
    "]\n",
    "dataset_folder = '/usr/local/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealSaver(DiagonalRepresentationGraphDataModule):\n",
    "    graphloader_class = RealGraphLoader\n",
    "    \n",
    "def prepare_model(model_path, hparams):\n",
    "    encoder = GraphEncoder(edge_encoder_class = MemoryEdgeEncoder, **hparams)\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    encoder_checkpoint = {\n",
    "        k.replace(\"encoder.edge_encoder.\", \"edge_encoder.\"): v\n",
    "        for (k, v) in checkpoint[\"state_dict\"].items()\n",
    "        if \"encoder\" in k\n",
    "    }\n",
    "    encoder.load_state_dict(encoder_checkpoint)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def prepare_dataset(dataset_path, hparams):\n",
    "    return RealSaver(\n",
    "        pickled_dataset_path=dataset_path,\n",
    "        use_labels=True,\n",
    "        bfs=True,\n",
    "        deduplicate_train = False,\n",
    "        deduplicate_val_test = False,\n",
    "        batch_size=32,\n",
    "        batch_size_val=32,\n",
    "        batch_size_test=32,\n",
    "        workers=0,\n",
    "        block_size=hparams['block_size'],\n",
    "        subgraph_scheduler_name='none',\n",
    "        subgraph_scheduler_params={}\n",
    "    )\n",
    "\n",
    "def get_embeddings(model, dataloader, add_features = False):\n",
    "    data_iterator = iter(dataloader)\n",
    "    X = []\n",
    "    Y = []\n",
    "    if add_features:\n",
    "        addidional_features = []\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(data_iterator)):\n",
    "        X.append(model(batch).detach().numpy())\n",
    "        Y.append(batch[3])\n",
    "        if add_features:\n",
    "            features = []\n",
    "            for j in range(batch[0].shape[0]):\n",
    "                A = diagonal_block_to_adj_matrix_representation(\n",
    "                    batch[0][j], batch[2][j]\n",
    "                )[:, :, 0].clamp(min = 0)\n",
    "                node_degress = (A + A.T).sum(axis = 0)\n",
    "                features.append([    \n",
    "                    batch[2][j],\n",
    "                    node_degress.sqrt().mean().detach().numpy(),\n",
    "                    node_degress.mean().detach().numpy(),\n",
    "                    node_degress.square().mean().detach().numpy()\n",
    "                ])\n",
    "\n",
    "            addidional_features.append(features)\n",
    "        \n",
    "    return np.concatenate(X), np.concatenate(Y), np.concatenate(addidional_features) if add_features else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04636dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_model(hparams_path, model_path, dataset_path, PCA_dim = None, add_features = False):\n",
    "    hparams = load_hparams(hparams_path)\n",
    "\n",
    "    model = prepare_model(model_path, hparams)\n",
    "    dataset = prepare_dataset(dataset_path, hparams)\n",
    "\n",
    "    train_X, train_Y, train_addidional_features = get_embeddings(\n",
    "        model, dataset.train_dataloader(shuffle=False), add_features = add_features\n",
    "    )\n",
    "    val_X, val_Y, val_addidional_features = get_embeddings(\n",
    "        model, dataset.val_dataloader(shuffle=False)[0], add_features = add_features\n",
    "    )\n",
    "    test_X, test_Y, test_addidional_features = get_embeddings(\n",
    "        model, dataset.test_dataloader(shuffle=False)[0], add_features = add_features\n",
    "    )\n",
    "\n",
    "    if PCA_dim is not None:\n",
    "        pca = PCA(n_components=PCA_dim)\n",
    "        pca.fit(train_X)\n",
    "        train_X = pca.transform(train_X)\n",
    "        val_X = pca.transform(val_X)\n",
    "        test_X = pca.transform(test_X)\n",
    "\n",
    "    if add_features:\n",
    "        mean_additional_features = train_addidional_features.mean(axis = 0)\n",
    "        train_X = np.concatenate([train_X, train_addidional_features/mean_additional_features], axis = 1)\n",
    "        val_X = np.concatenate([val_X, val_addidional_features/mean_additional_features], axis = 1)\n",
    "        test_X = np.concatenate([test_X, test_addidional_features/mean_additional_features], axis = 1)\n",
    "\n",
    "    sklearn_models = {\n",
    "        'NB': GaussianNB(),\n",
    "        'SVM': SVC(),\n",
    "        'Logistic regression': LogisticRegression(),\n",
    "        'xgboost': GradientBoostingClassifier(min_samples_leaf=10, verbose=False),\n",
    "    }\n",
    "\n",
    "    stats = {\n",
    "        'train': {},\n",
    "        'val': {},\n",
    "        'test': {}\n",
    "    }\n",
    "    for name, sklearn_model in sklearn_models.items():\n",
    "        sklearn_model.fit(train_X, train_Y)\n",
    "        train_preds = sklearn_model.predict(train_X)\n",
    "        val_preds = sklearn_model.predict(val_X)\n",
    "        test_preds = sklearn_model.predict(test_X)\n",
    "\n",
    "#         print(name)\n",
    "#         display(pd.DataFrame([val_Y, val_preds], index=['True', 'Pred']).transpose().value_counts())\n",
    "#         print(classification_report(val_Y, val_preds))\n",
    "        \n",
    "        stats['train'].update({\n",
    "            name: accuracy_score(train_Y, train_preds),\n",
    "        })\n",
    "        stats['val'].update({\n",
    "            name: accuracy_score(val_Y, val_preds),\n",
    "        })\n",
    "        stats['test'].update({\n",
    "            name: accuracy_score(test_Y, test_preds),\n",
    "        })\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ca9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results = {\n",
    "    'train':{dataset:{} for dataset in datasets}, \n",
    "    'val':{dataset:{} for dataset in datasets}, \n",
    "    'test':{dataset:{} for dataset in datasets}, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f573e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'IMDB-BINARY'\n",
    "add_features = True\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da35d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd2c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'IMDB-MULTI'\n",
    "add_features = True\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdddaca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'REDDIT-BINARY'\n",
    "add_features = True\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714cc64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe12f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'COLLAB'\n",
    "add_features = True\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'REDDIT-MULTI-5K'\n",
    "# for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "#     hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "#     model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "#     dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "#     stats = process_model(hparams_path, model_path, dataset_path)\n",
    "    \n",
    "#     selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "#     classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "#     selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "#     classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "#     selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "#     classification_results['test'][dataset_name].update(selected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a158844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'REDDIT-MULTI-12K'\n",
    "# for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "#     hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "#     model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "#     dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "#     stats = process_model(hparams_path, model_path, dataset_path)\n",
    "    \n",
    "#     selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "#     classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "#     selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "#     classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "#     selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "#     classification_results['test'][dataset_name].update(selected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca680af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff016b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce6619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d169a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705bff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a23c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2a075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d95191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de5a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddf0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c37224fb4409952eb251e31ba483053a30ac3ecaa917b50e887cf90f0c69f7d5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
