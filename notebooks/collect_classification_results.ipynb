{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920eef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.auto import tqdm\n",
    "from rga.data.diag_repr_graph_data_module import DiagonalRepresentationGraphDataModule\n",
    "from rga.data.graph_loaders import RealGraphLoader, SyntheticGraphLoader\n",
    "from rga.models.autoencoder_components import GraphEncoder\n",
    "from rga.models.edge_encoders import MemoryEdgeEncoder\n",
    "from rga.util.load_model import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from rga.util.adjmatrix.diagonal_block_representation import diagonal_block_to_adj_matrix_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_folder = ''\n",
    "datasets = [\n",
    "    'IMDB-BINARY',\n",
    "    'IMDB-MULTI',\n",
    "    'COLLAB',\n",
    "    'REDDIT-BINARY',\n",
    "    'REDDIT-MULTI-5K',\n",
    "    'REDDIT-MULTI-12K',\n",
    "]\n",
    "dataset_folder = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealSaver(DiagonalRepresentationGraphDataModule):\n",
    "    graphloader_class = RealGraphLoader\n",
    "    \n",
    "def prepare_model(model_path, hparams):\n",
    "    encoder = GraphEncoder(edge_encoder_class = MemoryEdgeEncoder, **hparams)\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    encoder_checkpoint = {\n",
    "        k.replace(\"encoder.edge_encoder.\", \"edge_encoder.\"): v\n",
    "        for (k, v) in checkpoint[\"state_dict\"].items()\n",
    "        if \"encoder\" in k\n",
    "    }\n",
    "    encoder.load_state_dict(encoder_checkpoint)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def prepare_dataset(dataset_path, hparams):\n",
    "    return RealSaver(\n",
    "        pickled_dataset_path=dataset_path,\n",
    "        use_labels=True,\n",
    "        bfs=True,\n",
    "        deduplicate_train = False,\n",
    "        deduplicate_val_test = False,\n",
    "        batch_size=32,\n",
    "        batch_size_val=32,\n",
    "        batch_size_test=32,\n",
    "        workers=0,\n",
    "        block_size=hparams['block_size'],\n",
    "        subgraph_scheduler_name='none',\n",
    "        subgraph_scheduler_params={}\n",
    "    )\n",
    "\n",
    "def get_embeddings(model, dataloader, add_features = False):\n",
    "    data_iterator = iter(dataloader)\n",
    "    X = []\n",
    "    Y = []\n",
    "    if add_features:\n",
    "        addidional_features = []\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(data_iterator, desc='Embeddings')):\n",
    "        X.append(model(batch).detach().numpy())\n",
    "        Y.append(batch[3])\n",
    "        if add_features:\n",
    "            features = []\n",
    "            for j in range(batch[0].shape[0]):\n",
    "                A = diagonal_block_to_adj_matrix_representation(\n",
    "                    batch[0][j], batch[2][j]\n",
    "                )[:, :, 0].clamp(min = 0)\n",
    "                node_degress = (A + A.T).sum(axis = 0)\n",
    "                features.append([    \n",
    "                    batch[2][j],\n",
    "                    np.power(node_degress.sqrt().mean().detach().numpy(), 2),\n",
    "                    node_degress.mean().detach().numpy(),\n",
    "                    np.sqrt(node_degress.square().mean().detach().numpy())\n",
    "                ])\n",
    "\n",
    "            addidional_features.append(features)\n",
    "        \n",
    "    return np.concatenate(X), np.concatenate(Y), np.concatenate(addidional_features) if add_features else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04636dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_model(hparams_path, model_path, dataset_path, PCA_dim = None, add_features = False):\n",
    "    hparams = load_hparams(hparams_path)\n",
    "\n",
    "    model = prepare_model(model_path, hparams)\n",
    "    dataset = prepare_dataset(dataset_path, hparams)\n",
    "\n",
    "    train_X, train_Y, train_addidional_features = get_embeddings(\n",
    "        model, dataset.train_dataloader(shuffle=False), add_features = add_features\n",
    "    )\n",
    "    val_X, val_Y, val_addidional_features = get_embeddings(\n",
    "        model, dataset.val_dataloader(shuffle=False)[0], add_features = add_features\n",
    "    )\n",
    "    test_X, test_Y, test_addidional_features = get_embeddings(\n",
    "        model, dataset.test_dataloader(shuffle=False)[0], add_features = add_features\n",
    "    )\n",
    "\n",
    "    if PCA_dim is not None:\n",
    "        pca = PCA(n_components=PCA_dim)\n",
    "        pca.fit(train_X)\n",
    "        train_X = pca.transform(train_X)\n",
    "        val_X = pca.transform(val_X)\n",
    "        test_X = pca.transform(test_X)\n",
    "\n",
    "    if add_features:\n",
    "        mean_additional_features = train_addidional_features.mean(axis = 0)\n",
    "        train_X_augmented = np.concatenate([train_X, train_addidional_features/mean_additional_features], axis = 1)\n",
    "        val_X_augmented = np.concatenate([val_X, val_addidional_features/mean_additional_features], axis = 1)\n",
    "        test_X_augmented = np.concatenate([test_X, test_addidional_features/mean_additional_features], axis = 1)\n",
    "        \n",
    "        train_X_only_stats = train_addidional_features/mean_additional_features\n",
    "        val_X_only_stats = val_addidional_features/mean_additional_features\n",
    "        test_X_only_stats = test_addidional_features/mean_additional_features\n",
    "        \n",
    "    sklearn_models = {\n",
    "        'NB': GaussianNB(),\n",
    "        'SVM': SVC(),\n",
    "        'Logistic regression': LogisticRegression(), \n",
    "        'xgboost': GradientBoostingClassifier(min_samples_leaf=20, verbose=False), \n",
    "        'Random forset': RandomForestClassifier(min_samples_leaf=20, verbose=False), \n",
    "    }\n",
    "\n",
    "    stats = {\n",
    "        'train': {},\n",
    "        'val': {},\n",
    "        'test': {}\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for mode, train_features, val_features, test_features in tqdm([\n",
    "        (' normal', train_X, val_X, test_X),\n",
    "        (' only_stats', train_X_only_stats, val_X_only_stats, test_X_only_stats),\n",
    "        (' augmented', train_X_augmented, val_X_augmented, test_X_augmented),\n",
    "\n",
    "    ], desc='Modes'):\n",
    "        for name, sklearn_model in tqdm(sklearn_models.items(), desc='Models', leave=True):\n",
    "            sklearn_model.fit(train_features, train_Y)\n",
    "            train_preds = sklearn_model.predict(train_features)\n",
    "            val_preds = sklearn_model.predict(val_features)\n",
    "            test_preds = sklearn_model.predict(test_features)\n",
    "        \n",
    "            stats['train'].update({\n",
    "                name + mode: accuracy_score(train_Y, train_preds),\n",
    "            })\n",
    "            stats['val'].update({\n",
    "                name + mode: accuracy_score(val_Y, val_preds),\n",
    "            })\n",
    "            stats['test'].update({\n",
    "                name + mode: accuracy_score(test_Y, test_preds),\n",
    "            })\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ca9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results = {\n",
    "    'train':{dataset:{} for dataset in datasets}, \n",
    "    'val':{dataset:{} for dataset in datasets}, \n",
    "    'test':{dataset:{} for dataset in datasets}, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f573e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'IMDB-BINARY'\n",
    "add_features = True\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features)\n",
    "    \n",
    "    for dset in ['train', 'val', 'test']:\n",
    "        selected_stats = {(pickle_file, k):v for (k,v) in stats[dset].items()}\n",
    "        classification_results[dset][dataset_name].update(selected_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd2c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'IMDB-MULTI'\n",
    "add_features = True\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05a14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'REDDIT-BINARY'\n",
    "add_features = True\n",
    "PCA_dim = None\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features, PCA_dim=PCA_dim)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe12f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'COLLAB'\n",
    "add_features = True\n",
    "PCA_dim = None\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features, PCA_dim=PCA_dim)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38776c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_name = 'REDDIT-MULTI-5K'\n",
    "add_features = True\n",
    "PCA_dim = None\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features, PCA_dim=PCA_dim)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a158844",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'REDDIT-MULTI-12K'\n",
    "add_features = True\n",
    "PCA_dim = None\n",
    "for pickle_file in tqdm([0, 1, 2, 3, 4]):\n",
    "    hparams_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '_hparams.yaml'\n",
    "    model_path = checkpoints_folder + dataset_name + '/' + str(pickle_file) + '.ckpt'\n",
    "    dataset_path = dataset_folder + dataset_name + '/' + str(pickle_file) + '.pkl'\n",
    "    \n",
    "    stats = process_model(hparams_path, model_path, dataset_path, add_features=add_features, PCA_dim=PCA_dim)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['train'].items()}\n",
    "    classification_results['train'][dataset_name].update(selected_stats)\n",
    "    \n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['val'].items()}\n",
    "    classification_results['val'][dataset_name].update(selected_stats)\n",
    "\n",
    "    selected_stats = {(pickle_file, k):v for (k,v) in stats['test'].items()}\n",
    "    classification_results['test'][dataset_name].update(selected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in ['train', 'val', 'test']:\n",
    "    print('='*40+'   ' + dset + '   ' + '='*40)\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            classification_results[dset]\n",
    "        ).reset_index().groupby('level_1').agg(['mean', 'std']).drop('level_0', axis = 1).round(3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classification_results.backup', 'wb') as f:\n",
    "    pickle.dump(classification_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d62a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c37224fb4409952eb251e31ba483053a30ac3ecaa917b50e887cf90f0c69f7d5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
